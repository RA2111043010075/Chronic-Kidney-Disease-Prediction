{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2764486,"sourceType":"datasetVersion","datasetId":1686903}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:15:41.896454Z","iopub.execute_input":"2025-01-24T13:15:41.896702Z","iopub.status.idle":"2025-01-24T13:16:03.663442Z","shell.execute_reply.started":"2025-01-24T13:15:41.896677Z","shell.execute_reply":"2025-01-24T13:16:03.662456Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install split-folders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:03:13.789273Z","iopub.execute_input":"2025-01-25T10:03:13.789483Z","iopub.status.idle":"2025-01-25T10:03:18.425611Z","shell.execute_reply.started":"2025-01-25T10:03:13.789462Z","shell.execute_reply":"2025-01-25T10:03:18.424609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import splitfolders\nsplitfolders.ratio('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone', output=\"output\", seed=1337, ratio=(.8, 0.1,0.1)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:03:22.400178Z","iopub.execute_input":"2025-01-25T10:03:22.400480Z","iopub.status.idle":"2025-01-25T10:04:57.540805Z","shell.execute_reply.started":"2025-01-25T10:03:22.400452Z","shell.execute_reply":"2025-01-25T10:04:57.540032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Import Libraries and Setup**","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Helper function to set random seeds for reproducibility\ndef set_all_seeds(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:10.302945Z","iopub.execute_input":"2025-01-25T10:06:10.303247Z","iopub.status.idle":"2025-01-25T10:06:15.438486Z","shell.execute_reply.started":"2025-01-25T10:06:10.303224Z","shell.execute_reply":"2025-01-25T10:06:15.437625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Importing necessary libraries (PyTorch, torchvision, numpy, etc.)\n* A helper function set_all_seeds() is used to ensure reproducibility by setting random seeds for all libraries.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Define Vision Transformer (ViT) Model**","metadata":{}},{"cell_type":"code","source":"def get_vit(seed, class_names, device):\n    set_all_seeds(seed)\n    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    vit_transforms = vit_weights.transforms()\n    vit = torchvision.models.vit_b_16(weights=vit_weights).to(device)\n    \n    # Freeze the model parameters\n    for param in vit.parameters():\n        param.requires_grad = False\n    \n    # Modify the classifier layer\n    vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n    \n    return vit, vit_transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:26.002159Z","iopub.execute_input":"2025-01-25T10:06:26.002722Z","iopub.status.idle":"2025-01-25T10:06:26.008574Z","shell.execute_reply.started":"2025-01-25T10:06:26.002690Z","shell.execute_reply":"2025-01-25T10:06:26.007561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Vision Transformer (ViT): This function loads a pre-trained ViT model and customizes the final classifier (vit.heads) to match the number of output classes.\n* Freezing the parameters in the pretrained layers (except for the classifier) to focus on training only the classifier.\n","metadata":{}},{"cell_type":"markdown","source":"**Train the Model**","metadata":{}},{"cell_type":"code","source":"import os\n\ndef train_model_with_early_stopping(model, train_loader, val_loader, test_loader, device, num_epochs, patience=5):\n    \"\"\"\n    Train a model with early stopping and learning rate scheduling.\n    \n    Parameters:\n        model: The neural network to train.\n        train_loader: DataLoader for the training set.\n        val_loader: DataLoader for the validation set.\n        test_loader: DataLoader for the testing set.\n        device: The device to train on (CPU or GPU).\n        num_epochs: Maximum number of epochs.\n        patience: Number of epochs to wait for improvement before stopping early.\n    \n    Returns:\n        train_loss_list: List of training losses per epoch.\n        train_acc_list: List of training accuracies per epoch.\n        val_acc_list: List of validation accuracies per epoch.\n        val_loss_list: List of validation losses per epoch.\n    \"\"\"\n    # Optimizer, Scheduler, and Loss\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Metrics tracking\n    train_loss_list, train_acc_list, val_loss_list, val_acc_list = [], [], [], []\n    best_val_loss = float('inf')  # Initial value for best validation loss\n    patience_counter = 5         # Counter for early stopping\n    best_model_path = 'best_model.pt'\n\n    # Training loop\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        \n        # Training pass\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward and backward passes\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # Track metrics\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        # Calculate training loss and accuracy\n        train_loss = running_loss / len(train_loader)\n        train_acc = 100 * correct / total\n        train_loss_list.append(train_loss)\n        train_acc_list.append(train_acc)\n\n        # Validation pass\n        val_loss, val_acc = evaluate_validation(model, val_loader, criterion, device)\n        val_loss_list.append(val_loss)\n        val_acc_list.append(val_acc)\n\n        # Log progress\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), best_model_path)  # Save the best model\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping activated after {epoch+1} epochs.\")\n            break\n        \n        # Step the scheduler\n        scheduler.step()\n\n    # End of training\n    print(f\"Training complete in {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Load the best model for evaluation\n    model.load_state_dict(torch.load(best_model_path))\n    test_acc = compute_accuracy(model, test_loader, device)\n    print(f\"Test Accuracy: {test_acc:.2f}%\")\n    \n    # Clean up saved model file\n    if os.path.exists(best_model_path):\n        os.remove(best_model_path)\n\n    return train_loss_list, train_acc_list, val_acc_list, val_loss_list\n\ndef evaluate_validation(model, loader, criterion, device):\n    \"\"\"\n    Evaluate the model on the validation dataset.\n    \n    Parameters:\n        model: The model to evaluate.\n        loader: DataLoader for the validation/test set.\n        criterion: Loss function.\n        device: Device to perform computations on.\n    \n    Returns:\n        val_loss: Validation loss.\n        val_acc: Validation accuracy.\n    \"\"\"\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_loss = running_loss / len(loader)\n    val_acc = 100 * correct / total\n    return val_loss, val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:33.847782Z","iopub.execute_input":"2025-01-25T10:06:33.848104Z","iopub.status.idle":"2025-01-25T10:06:33.859384Z","shell.execute_reply.started":"2025-01-25T10:06:33.848075Z","shell.execute_reply":"2025-01-25T10:06:33.858462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function trains the model with a specified number of epochs and tracks the training/validation loss and accuracy.\nUses Stochastic Gradient Descent (SGD) with momentum for optimization and StepLR for learning rate scheduling.\nEarly Stopping is implemented to stop training when validation loss doesn’t improve for several epochs.","metadata":{}},{"cell_type":"markdown","source":"**Evaluation Functions**","metadata":{}},{"cell_type":"code","source":"# Evaluation function for validation\ndef evaluate_validation(model, val_loader, criterion, device):\n    model.eval()\n    correct, total = 0, 0\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_loss /= len(val_loader)\n    val_acc = 100 * correct / total\n    return val_loss, val_acc\n\n# Compute accuracy on a dataset\ndef compute_accuracy(model, data_loader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return 100 * correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:44.440624Z","iopub.execute_input":"2025-01-25T10:06:44.440933Z","iopub.status.idle":"2025-01-25T10:06:44.447343Z","shell.execute_reply.started":"2025-01-25T10:06:44.440909Z","shell.execute_reply":"2025-01-25T10:06:44.446524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Evaluation: These functions evaluate the model on a validation or test set and compute loss and accuracy.\n* compute_accuracy() computes the overall accuracy of the model on any dataset (training, validation, or test).\n\n","metadata":{}},{"cell_type":"markdown","source":"**Plotting the Training and Validation Metrics**","metadata":{}},{"cell_type":"code","source":"# Function to plot training and validation metrics\ndef plot_metrics(train_loss, val_loss, train_acc, val_acc):\n    epochs = range(1, len(train_loss) + 1)\n    plt.figure(figsize=(12, 6))\n    \n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n    plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_acc, label='Train Accuracy', marker='o')\n    plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:54.270529Z","iopub.execute_input":"2025-01-25T10:06:54.270853Z","iopub.status.idle":"2025-01-25T10:06:54.276094Z","shell.execute_reply.started":"2025-01-25T10:06:54.270827Z","shell.execute_reply":"2025-01-25T10:06:54.275371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function visualizes the training and validation loss and accuracy over epochs. It provides insights into how well the model is training and whether it's overfitting.","metadata":{}},{"cell_type":"markdown","source":"**Load Data and Train the Model**","metadata":{}},{"cell_type":"code","source":"# Define parameters\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nnum_epochs = 30\nbatch_size = 32\ntrain_dir = '/kaggle/working/output/train/'\nval_dir = '/kaggle/working/output/val/'\ntest_dir = '/kaggle/working/output/test/'\n\n# Load dataset\ntransformers = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_data = datasets.ImageFolder(train_dir, transform=transformers)\nval_data = datasets.ImageFolder(val_dir, transform=transformers)\ntest_data = datasets.ImageFolder(test_dir, transform=transformers)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nclass_names = train_data.classes\nmodel, transformers = get_vit(42, class_names, DEVICE)\n\n# Train the model and get loss/accuracy values\ntrain_loss, train_acc, val_acc, val_loss = train_model_with_early_stopping(model, train_loader, val_loader, test_loader, DEVICE, num_epochs)\n\n# Plot the metrics including loss\nplot_metrics(train_loss, val_loss, train_acc, val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:07:08.122916Z","iopub.execute_input":"2025-01-25T10:07:08.123193Z","iopub.status.idle":"2025-01-25T11:18:17.793532Z","shell.execute_reply.started":"2025-01-25T10:07:08.123172Z","shell.execute_reply":"2025-01-25T11:18:17.792537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport seaborn as sns\n\n# Confusion Matrix for Test Dataset\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.title('Test Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Classification Report and Metrics\nreport = classification_report(all_labels, all_preds, target_names=class_names)\nprint(\"Classification Report:\\n\")\nprint(report)\n\n# Overall Metrics\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision (Weighted): {precision:.4f}\")\nprint(f\"Recall (Weighted): {recall:.4f}\")\nprint(f\"F1-Score (Weighted): {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:43:27.630980Z","iopub.execute_input":"2025-01-25T11:43:27.631303Z","iopub.status.idle":"2025-01-25T11:43:43.345830Z","shell.execute_reply.started":"2025-01-25T11:43:27.631279Z","shell.execute_reply":"2025-01-25T11:43:43.344973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\ndef compute_overall_auc(model, loader, class_names, device):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.nn.Softmax(dim=1)(outputs)  # Convert logits to probabilities\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    \n    # Binarize labels for multi-class AUC\n    all_labels_bin = label_binarize(all_labels, classes=np.arange(len(class_names)))\n    \n    # Calculate macro-average and weighted-average AUC\n    macro_auc = roc_auc_score(all_labels_bin, all_probs, average='macro', multi_class='ovr')\n    weighted_auc = roc_auc_score(all_labels_bin, all_probs, average='weighted', multi_class='ovr')\n    \n    print(f\"Overall Macro-Average AUC: {macro_auc:.4f}\")\n    print(f\"Overall Weighted-Average AUC: {weighted_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:50:13.743921Z","iopub.execute_input":"2025-01-25T11:50:13.744239Z","iopub.status.idle":"2025-01-25T11:50:13.750372Z","shell.execute_reply.started":"2025-01-25T11:50:13.744215Z","shell.execute_reply":"2025-01-25T11:50:13.749387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_overall_auc(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:50:39.042573Z","iopub.execute_input":"2025-01-25T11:50:39.042880Z","iopub.status.idle":"2025-01-25T11:50:53.457441Z","shell.execute_reply.started":"2025-01-25T11:50:39.042858Z","shell.execute_reply":"2025-01-25T11:50:53.456401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_overall_auc(model, loader, class_names, device):\n    model.eval()\n    all_probs, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.nn.Softmax(dim=1)(outputs)  # Convert logits to probabilities\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    \n    # Binarize labels for multi-class ROC\n    all_labels_bin = label_binarize(all_labels, classes=np.arange(len(class_names)))\n    \n    # Initialize plot\n    plt.figure(figsize=(10, 8))\n    colors = ['blue', 'green', 'red', 'orange']  # Colors for each class\n    fpr_dict, tpr_dict, roc_auc_dict = {}, {}, {}\n    \n    for i, class_name in enumerate(class_names):\n        fpr_dict[i], tpr_dict[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n        roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n        plt.plot(fpr_dict[i], tpr_dict[i], color=colors[i], lw=2, \n                 label=f'Class {class_name} (AUC = {roc_auc_dict[i]:.2f})')\n    \n    # Compute macro-average ROC curve and AUC\n    fpr_macro, tpr_macro, _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n    roc_auc_macro = auc(fpr_macro, tpr_macro)\n    plt.plot(fpr_macro, tpr_macro, color='darkviolet', linestyle='--', lw=2, \n             label=f'Macro-Average ROC (AUC = {roc_auc_macro:.2f})')\n    \n    # Plot settings\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve for All Classes')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:51:43.309904Z","iopub.execute_input":"2025-01-25T11:51:43.310227Z","iopub.status.idle":"2025-01-25T11:51:43.318743Z","shell.execute_reply.started":"2025-01-25T11:51:43.310204Z","shell.execute_reply":"2025-01-25T11:51:43.317849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_overall_auc(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:52:02.117347Z","iopub.execute_input":"2025-01-25T11:52:02.117704Z","iopub.status.idle":"2025-01-25T11:52:16.647341Z","shell.execute_reply.started":"2025-01-25T11:52:02.117674Z","shell.execute_reply":"2025-01-25T11:52:16.646433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_metrics(cm, class_names):\n    metrics = []\n    \n    # For each class, calculate TP, TN, FP, FN\n    for i, class_name in enumerate(class_names):\n        tp = cm[i, i]  # True positives are the diagonal elements\n        fn = cm[i, :].sum() - tp  # False negatives are the sum of the row minus TP\n        fp = cm[:, i].sum() - tp  # False positives are the sum of the column minus TP\n        tn = cm.sum() - (tp + fp + fn)  # True negatives are the sum of all elements minus TP, FP, and FN\n        \n        metrics.append({\n            'Class': class_name,\n            'TP': tp,\n            'TN': tn,\n            'FP': fp,\n            'FN': fn\n        })\n        \n        print(f\"Class: {class_name}\")\n        print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n    \n    return metrics\n\n# Example usage\ndef evaluate_and_calculate_metrics(model, loader, class_names, device):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    print(\"Confusion Matrix:\\n\", cm)\n    \n    metrics = calculate_metrics(cm, class_names)\n    \n    # Print the metrics in a more structured way (e.g., as a DataFrame)\n    import pandas as pd\n    metrics_df = pd.DataFrame(metrics)\n    print(metrics_df)\n    \n    return metrics_df\n\n# Assuming you have already trained the model and have a test loader and class names:\nmetrics_df = evaluate_and_calculate_metrics(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:02:21.433892Z","iopub.execute_input":"2025-01-25T12:02:21.434225Z","iopub.status.idle":"2025-01-25T12:02:35.842984Z","shell.execute_reply.started":"2025-01-25T12:02:21.434203Z","shell.execute_reply":"2025-01-25T12:02:35.841861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict_single_image(image_path, model, transform, class_names, device):\n    \"\"\"\n    Predict the class of a single image.\n\n    Parameters:\n        image_path (str): Path to the image file.\n        model (torch.nn.Module): Trained model.\n        transform (torchvision.transforms): Image preprocessing pipeline.\n        class_names (list): List of class names.\n        device (torch.device): Device to run the model on.\n\n    Returns:\n        predicted_class (str): Predicted class name.\n    \"\"\"\n    model.eval()\n\n    # Load and preprocess the image\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(image_tensor)\n        _, predicted = torch.max(output, 1)\n        predicted_class = class_names[predicted.item()]\n\n    return predicted_class\n\n# Example usage:\nsample_image_path1 = \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Normal/Normal- (1001).jpg\"  # <- Change this to your image path\npredicted_class1 = predict_single_image(sample_image_path1, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path1)}': {predicted_class1}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path2= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Tumor/Tumor- (1007).jpg\"\npredicted_class2 = predict_single_image(sample_image_path2, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path2)}': {predicted_class2}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path3= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1039).jpg\"\npredicted_class3 = predict_single_image(sample_image_path3, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path3)}': {predicted_class3}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path4= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Stone/Stone- (1012).jpg\"\npredicted_class4 = predict_single_image(sample_image_path4, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path4)}': {predicted_class4}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}