{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2764486,"sourceType":"datasetVersion","datasetId":1686903}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:15:41.896454Z","iopub.execute_input":"2025-01-24T13:15:41.896702Z","iopub.status.idle":"2025-01-24T13:16:03.663442Z","shell.execute_reply.started":"2025-01-24T13:15:41.896677Z","shell.execute_reply":"2025-01-24T13:16:03.662456Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install split-folders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:03:13.789273Z","iopub.execute_input":"2025-01-25T10:03:13.789483Z","iopub.status.idle":"2025-01-25T10:03:18.425611Z","shell.execute_reply.started":"2025-01-25T10:03:13.789462Z","shell.execute_reply":"2025-01-25T10:03:18.424609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import splitfolders\nsplitfolders.ratio('/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone', output=\"output\", seed=1337, ratio=(.8, 0.1,0.1)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:03:22.400178Z","iopub.execute_input":"2025-01-25T10:03:22.400480Z","iopub.status.idle":"2025-01-25T10:04:57.540805Z","shell.execute_reply.started":"2025-01-25T10:03:22.400452Z","shell.execute_reply":"2025-01-25T10:04:57.540032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Import Libraries and Setup**","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Helper function to set random seeds for reproducibility\ndef set_all_seeds(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:10.302945Z","iopub.execute_input":"2025-01-25T10:06:10.303247Z","iopub.status.idle":"2025-01-25T10:06:15.438486Z","shell.execute_reply.started":"2025-01-25T10:06:10.303224Z","shell.execute_reply":"2025-01-25T10:06:15.437625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Importing necessary libraries (PyTorch, torchvision, numpy, etc.)\n* A helper function set_all_seeds() is used to ensure reproducibility by setting random seeds for all libraries.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Define Vision Transformer (ViT) Model**","metadata":{}},{"cell_type":"code","source":"def get_vit(seed, class_names, device):\n    set_all_seeds(seed)\n    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    vit_transforms = vit_weights.transforms()\n    vit = torchvision.models.vit_b_16(weights=vit_weights).to(device)\n    \n    # Freeze the model parameters\n    for param in vit.parameters():\n        param.requires_grad = False\n    \n    # Modify the classifier layer\n    vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n    \n    return vit, vit_transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:26.002159Z","iopub.execute_input":"2025-01-25T10:06:26.002722Z","iopub.status.idle":"2025-01-25T10:06:26.008574Z","shell.execute_reply.started":"2025-01-25T10:06:26.002690Z","shell.execute_reply":"2025-01-25T10:06:26.007561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Vision Transformer (ViT): This function loads a pre-trained ViT model and customizes the final classifier (vit.heads) to match the number of output classes.\n* Freezing the parameters in the pretrained layers (except for the classifier) to focus on training only the classifier.\n","metadata":{}},{"cell_type":"markdown","source":"**Train the Model**","metadata":{}},{"cell_type":"code","source":"import os\n\ndef train_model_with_early_stopping(model, train_loader, val_loader, test_loader, device, num_epochs, patience=5):\n    \"\"\"\n    Train a model with early stopping and learning rate scheduling.\n    \n    Parameters:\n        model: The neural network to train.\n        train_loader: DataLoader for the training set.\n        val_loader: DataLoader for the validation set.\n        test_loader: DataLoader for the testing set.\n        device: The device to train on (CPU or GPU).\n        num_epochs: Maximum number of epochs.\n        patience: Number of epochs to wait for improvement before stopping early.\n    \n    Returns:\n        train_loss_list: List of training losses per epoch.\n        train_acc_list: List of training accuracies per epoch.\n        val_acc_list: List of validation accuracies per epoch.\n        val_loss_list: List of validation losses per epoch.\n    \"\"\"\n    # Optimizer, Scheduler, and Loss\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Metrics tracking\n    train_loss_list, train_acc_list, val_loss_list, val_acc_list = [], [], [], []\n    best_val_loss = float('inf')  # Initial value for best validation loss\n    patience_counter = 5         # Counter for early stopping\n    best_model_path = 'best_model.pt'\n\n    # Training loop\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n        \n        # Training pass\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # Forward and backward passes\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # Track metrics\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        # Calculate training loss and accuracy\n        train_loss = running_loss / len(train_loader)\n        train_acc = 100 * correct / total\n        train_loss_list.append(train_loss)\n        train_acc_list.append(train_acc)\n\n        # Validation pass\n        val_loss, val_acc = evaluate_validation(model, val_loader, criterion, device)\n        val_loss_list.append(val_loss)\n        val_acc_list.append(val_acc)\n\n        # Log progress\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), best_model_path)  # Save the best model\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping activated after {epoch+1} epochs.\")\n            break\n        \n        # Step the scheduler\n        scheduler.step()\n\n    # End of training\n    print(f\"Training complete in {(time.time() - start_time)/60:.2f} minutes\")\n\n    # Load the best model for evaluation\n    model.load_state_dict(torch.load(best_model_path))\n    test_acc = compute_accuracy(model, test_loader, device)\n    print(f\"Test Accuracy: {test_acc:.2f}%\")\n    \n    # Clean up saved model file\n    if os.path.exists(best_model_path):\n        os.remove(best_model_path)\n\n    return train_loss_list, train_acc_list, val_acc_list, val_loss_list\n\ndef evaluate_validation(model, loader, criterion, device):\n    \"\"\"\n    Evaluate the model on the validation dataset.\n    \n    Parameters:\n        model: The model to evaluate.\n        loader: DataLoader for the validation/test set.\n        criterion: Loss function.\n        device: Device to perform computations on.\n    \n    Returns:\n        val_loss: Validation loss.\n        val_acc: Validation accuracy.\n    \"\"\"\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_loss = running_loss / len(loader)\n    val_acc = 100 * correct / total\n    return val_loss, val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:33.847782Z","iopub.execute_input":"2025-01-25T10:06:33.848104Z","iopub.status.idle":"2025-01-25T10:06:33.859384Z","shell.execute_reply.started":"2025-01-25T10:06:33.848075Z","shell.execute_reply":"2025-01-25T10:06:33.858462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function trains the model with a specified number of epochs and tracks the training/validation loss and accuracy.\nUses Stochastic Gradient Descent (SGD) with momentum for optimization and StepLR for learning rate scheduling.\nEarly Stopping is implemented to stop training when validation loss doesnâ€™t improve for several epochs.","metadata":{}},{"cell_type":"markdown","source":"**Evaluation Functions**","metadata":{}},{"cell_type":"code","source":"# Evaluation function for validation\ndef evaluate_validation(model, val_loader, criterion, device):\n    model.eval()\n    correct, total = 0, 0\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_loss /= len(val_loader)\n    val_acc = 100 * correct / total\n    return val_loss, val_acc\n\n# Compute accuracy on a dataset\ndef compute_accuracy(model, data_loader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return 100 * correct / total\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:44.440624Z","iopub.execute_input":"2025-01-25T10:06:44.440933Z","iopub.status.idle":"2025-01-25T10:06:44.447343Z","shell.execute_reply.started":"2025-01-25T10:06:44.440909Z","shell.execute_reply":"2025-01-25T10:06:44.446524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Evaluation: These functions evaluate the model on a validation or test set and compute loss and accuracy.\n* compute_accuracy() computes the overall accuracy of the model on any dataset (training, validation, or test).\n\n","metadata":{}},{"cell_type":"markdown","source":"**Plotting the Training and Validation Metrics**","metadata":{}},{"cell_type":"code","source":"# Function to plot training and validation metrics\ndef plot_metrics(train_loss, val_loss, train_acc, val_acc):\n    epochs = range(1, len(train_loss) + 1)\n    plt.figure(figsize=(12, 6))\n    \n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n    plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    \n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_acc, label='Train Accuracy', marker='o')\n    plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:06:54.270529Z","iopub.execute_input":"2025-01-25T10:06:54.270853Z","iopub.status.idle":"2025-01-25T10:06:54.276094Z","shell.execute_reply.started":"2025-01-25T10:06:54.270827Z","shell.execute_reply":"2025-01-25T10:06:54.275371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This function visualizes the training and validation loss and accuracy over epochs. It provides insights into how well the model is training and whether it's overfitting.","metadata":{}},{"cell_type":"markdown","source":"**Load Data and Train the Model**","metadata":{}},{"cell_type":"code","source":"# Define parameters\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nnum_epochs = 30\nbatch_size = 32\ntrain_dir = '/kaggle/working/output/train/'\nval_dir = '/kaggle/working/output/val/'\ntest_dir = '/kaggle/working/output/test/'\n\n# Load dataset\ntransformers = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_data = datasets.ImageFolder(train_dir, transform=transformers)\nval_data = datasets.ImageFolder(val_dir, transform=transformers)\ntest_data = datasets.ImageFolder(test_dir, transform=transformers)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nclass_names = train_data.classes\nmodel, transformers = get_vit(42, class_names, DEVICE)\n\n# Train the model and get loss/accuracy values\ntrain_loss, train_acc, val_acc, val_loss = train_model_with_early_stopping(model, train_loader, val_loader, test_loader, DEVICE, num_epochs)\n\n# Plot the metrics including loss\nplot_metrics(train_loss, val_loss, train_acc, val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:07:08.122916Z","iopub.execute_input":"2025-01-25T10:07:08.123193Z","iopub.status.idle":"2025-01-25T11:18:17.793532Z","shell.execute_reply.started":"2025-01-25T10:07:08.123172Z","shell.execute_reply":"2025-01-25T11:18:17.792537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport seaborn as sns\n\n# Confusion Matrix for Test Dataset\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.title('Test Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Classification Report and Metrics\nreport = classification_report(all_labels, all_preds, target_names=class_names)\nprint(\"Classification Report:\\n\")\nprint(report)\n\n# Overall Metrics\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision (Weighted): {precision:.4f}\")\nprint(f\"Recall (Weighted): {recall:.4f}\")\nprint(f\"F1-Score (Weighted): {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:43:27.630980Z","iopub.execute_input":"2025-01-25T11:43:27.631303Z","iopub.status.idle":"2025-01-25T11:43:43.345830Z","shell.execute_reply.started":"2025-01-25T11:43:27.631279Z","shell.execute_reply":"2025-01-25T11:43:43.344973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\ndef compute_overall_auc(model, loader, class_names, device):\n    model.eval()\n    all_probs, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.nn.Softmax(dim=1)(outputs)  # Convert logits to probabilities\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    \n    # Binarize labels for multi-class AUC\n    all_labels_bin = label_binarize(all_labels, classes=np.arange(len(class_names)))\n    \n    # Calculate macro-average and weighted-average AUC\n    macro_auc = roc_auc_score(all_labels_bin, all_probs, average='macro', multi_class='ovr')\n    weighted_auc = roc_auc_score(all_labels_bin, all_probs, average='weighted', multi_class='ovr')\n    \n    print(f\"Overall Macro-Average AUC: {macro_auc:.4f}\")\n    print(f\"Overall Weighted-Average AUC: {weighted_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:50:13.743921Z","iopub.execute_input":"2025-01-25T11:50:13.744239Z","iopub.status.idle":"2025-01-25T11:50:13.750372Z","shell.execute_reply.started":"2025-01-25T11:50:13.744215Z","shell.execute_reply":"2025-01-25T11:50:13.749387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_overall_auc(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:50:39.042573Z","iopub.execute_input":"2025-01-25T11:50:39.042880Z","iopub.status.idle":"2025-01-25T11:50:53.457441Z","shell.execute_reply.started":"2025-01-25T11:50:39.042858Z","shell.execute_reply":"2025-01-25T11:50:53.456401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_overall_auc(model, loader, class_names, device):\n    model.eval()\n    all_probs, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            probs = torch.nn.Softmax(dim=1)(outputs)  # Convert logits to probabilities\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    all_probs = np.array(all_probs)\n    all_labels = np.array(all_labels)\n    \n    # Binarize labels for multi-class ROC\n    all_labels_bin = label_binarize(all_labels, classes=np.arange(len(class_names)))\n    \n    # Initialize plot\n    plt.figure(figsize=(10, 8))\n    colors = ['blue', 'green', 'red', 'orange']  # Colors for each class\n    fpr_dict, tpr_dict, roc_auc_dict = {}, {}, {}\n    \n    for i, class_name in enumerate(class_names):\n        fpr_dict[i], tpr_dict[i], _ = roc_curve(all_labels_bin[:, i], all_probs[:, i])\n        roc_auc_dict[i] = auc(fpr_dict[i], tpr_dict[i])\n        plt.plot(fpr_dict[i], tpr_dict[i], color=colors[i], lw=2, \n                 label=f'Class {class_name} (AUC = {roc_auc_dict[i]:.2f})')\n    \n    # Compute macro-average ROC curve and AUC\n    fpr_macro, tpr_macro, _ = roc_curve(all_labels_bin.ravel(), all_probs.ravel())\n    roc_auc_macro = auc(fpr_macro, tpr_macro)\n    plt.plot(fpr_macro, tpr_macro, color='darkviolet', linestyle='--', lw=2, \n             label=f'Macro-Average ROC (AUC = {roc_auc_macro:.2f})')\n    \n    # Plot settings\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve for All Classes')\n    plt.legend(loc='lower right')\n    plt.grid(alpha=0.3)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:51:43.309904Z","iopub.execute_input":"2025-01-25T11:51:43.310227Z","iopub.status.idle":"2025-01-25T11:51:43.318743Z","shell.execute_reply.started":"2025-01-25T11:51:43.310204Z","shell.execute_reply":"2025-01-25T11:51:43.317849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_overall_auc(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T11:52:02.117347Z","iopub.execute_input":"2025-01-25T11:52:02.117704Z","iopub.status.idle":"2025-01-25T11:52:16.647341Z","shell.execute_reply.started":"2025-01-25T11:52:02.117674Z","shell.execute_reply":"2025-01-25T11:52:16.646433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_metrics(cm, class_names):\n    metrics = []\n    \n    # For each class, calculate TP, TN, FP, FN\n    for i, class_name in enumerate(class_names):\n        tp = cm[i, i]  # True positives are the diagonal elements\n        fn = cm[i, :].sum() - tp  # False negatives are the sum of the row minus TP\n        fp = cm[:, i].sum() - tp  # False positives are the sum of the column minus TP\n        tn = cm.sum() - (tp + fp + fn)  # True negatives are the sum of all elements minus TP, FP, and FN\n        \n        metrics.append({\n            'Class': class_name,\n            'TP': tp,\n            'TN': tn,\n            'FP': fp,\n            'FN': fn\n        })\n        \n        print(f\"Class: {class_name}\")\n        print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n    \n    return metrics\n\n# Example usage\ndef evaluate_and_calculate_metrics(model, loader, class_names, device):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    print(\"Confusion Matrix:\\n\", cm)\n    \n    metrics = calculate_metrics(cm, class_names)\n    \n    # Print the metrics in a more structured way (e.g., as a DataFrame)\n    import pandas as pd\n    metrics_df = pd.DataFrame(metrics)\n    print(metrics_df)\n    \n    return metrics_df\n\n# Assuming you have already trained the model and have a test loader and class names:\nmetrics_df = evaluate_and_calculate_metrics(model, test_loader, train_data.classes, DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:02:21.433892Z","iopub.execute_input":"2025-01-25T12:02:21.434225Z","iopub.status.idle":"2025-01-25T12:02:35.842984Z","shell.execute_reply.started":"2025-01-25T12:02:21.434203Z","shell.execute_reply":"2025-01-25T12:02:35.841861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict_single_image(image_path, model, transform, class_names, device):\n    \"\"\"\n    Predict the class of a single image.\n\n    Parameters:\n        image_path (str): Path to the image file.\n        model (torch.nn.Module): Trained model.\n        transform (torchvision.transforms): Image preprocessing pipeline.\n        class_names (list): List of class names.\n        device (torch.device): Device to run the model on.\n\n    Returns:\n        predicted_class (str): Predicted class name.\n    \"\"\"\n    model.eval()\n\n    # Load and preprocess the image\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(image_tensor)\n        _, predicted = torch.max(output, 1)\n        predicted_class = class_names[predicted.item()]\n\n    return predicted_class\n\n# Example usage:\nsample_image_path1 = \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Normal/Normal- (1001).jpg\"  # <- Change this to your image path\npredicted_class1 = predict_single_image(sample_image_path1, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path1)}': {predicted_class1}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path2= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Tumor/Tumor- (1007).jpg\"\npredicted_class2 = predict_single_image(sample_image_path2, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path2)}': {predicted_class2}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path3= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst/Cyst- (1039).jpg\"\npredicted_class3 = predict_single_image(sample_image_path3, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path3)}': {predicted_class3}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_path4= \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Stone/Stone- (1012).jpg\"\npredicted_class4 = predict_single_image(sample_image_path4, model, transformers, class_names, DEVICE)\nprint(f\"Predicted class for the image '{os.path.basename(sample_image_path4)}': {predicted_class4}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}